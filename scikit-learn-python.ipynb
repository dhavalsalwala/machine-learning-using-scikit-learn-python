{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_zHfyv5avXr"
   },
   "source": [
    "# 1) Machine Learning Library: scikit-learn\n",
    "\n",
    "Scikit-learn is an open source machine learning library for the Python programming language. It is mainly written in python with the exception of few core algorithms which are written in Cython to achieve performances. It makes heavy use of the other python packages: NumPy, SciPy, and matplotlib.\n",
    "\n",
    "[Scikit-learn GitHub Repo](https://github.com/scikit-learn/scikit-learn)\n",
    "\n",
    "Features of Scikit-learn:\n",
    "\n",
    "* Clean and Streamlined APIs explained with complete documentation and examples.\n",
    "* Simple and efficient tools for data mining and data analysis. Once you understand the basic use of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward.\n",
    "* Easily availability and reusable in various contexts. Clean integration with other python APIs.\n",
    "* Open source and commercially usable - BSD license.\n",
    "* Provide a platform to design similar projects as Scikit-learn via [GitHub community](https://github.com/scikit-learn-contrib/scikit-learn-contrib)\n",
    "\n",
    "**Why Scikit-learn is appropriate for this task ?**\n",
    "\n",
    "* Very Efficient for supervised learning task like classification.\n",
    "* Simple and handy implementation of many classification based algorithms.\n",
    "* Implementation can easily be integrated with other python packages for better representation of the output. Viz. Pandas, Mathplotlib, NumPy\n",
    "* Lucidly written estimator APIs with same import/instantiate/fit/predict pattern holds. Makes it easy to learn and build estimations. \n",
    "\n",
    "# 2) Data Preparation Step\n",
    "\n",
    "* Reading data set (autoimmune.txt) using Python package: Pandas\n",
    "* Using read_csv function. Columns are separated by tabs and rows are separated by newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLsk1U5oavX5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv('data/autoimmune.txt', delimiter=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwZBltm2avYi"
   },
   "source": [
    "* Apply transpose function to map attributes as columns and patient as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtpX8jDKavY0"
   },
   "outputs": [],
   "source": [
    "ds=ds.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCYwE3x-avZ_"
   },
   "source": [
    "* Assingning names to the columns as per the data set\n",
    "* Re-numbering data set index from 1 (using numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmjtuGrNavaG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ds.columns=['Age','Blood_Pressure','BMI','Plasma_level','Autoimmune_Disease','Adverse_events','Drug_in_serum','Liver_function','Activity_test','Secondary_test']\n",
    "ds.index = np.arange(1, len(ds) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dx4hwovavaW"
   },
   "source": [
    "* Mapping data set into dependent attributes and class label attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MO_sj9iavaa"
   },
   "outputs": [],
   "source": [
    "X = ds.drop('Autoimmune_Disease',axis=1)\n",
    "y = ds['Autoimmune_Disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DI-n_YFEavbb"
   },
   "source": [
    "# 3) Classification Alogrithm \n",
    "### 1: C4.5 / ID3\n",
    "A decision tree is a tree where each node specifies a criteria test of some hypothesis, and each branch descending from the node corresponds to one of the possible values resulting from that criteria. Each branch represents a decision and each leaf represents an outcome(categorical or continues value). It belongs to the family of supervised learning algorithms. It can be used for solving regression and classification problems. \n",
    "\n",
    "The main objective of decision tree algorithm is to build a hypothesis from the given training data and use these decision rules to predict class label in other cases. For predicting a class label, one start from the root of the tree. Then comparing the values of the root attribute with record’s attribute. On the basis of comparison, follow the branch corresponding to that value and jump to the next node. Continuing on the same approach, one recursively compare record’s attribute values with other internal nodes of the tree until a leaf node with the class label is found.\n",
    "\n",
    "**Note: The central idea of ID3 algorithm is selecting which attribute to test at each node in the tree:** \n",
    "\n",
    "* There are different selection measure to identify the attribute which can be considered as the root note at each level.\n",
    "    * Information gain - for categorical attributes\n",
    "        * In order to define information gain precisely, we begin by defining a entropy - a measure of the uncertainty of a random variable.\n",
    "    * Gini index - for continous attributes\n",
    "    \n",
    "    \n",
    "* ID3 Search technique:\n",
    "    * Greedy search\n",
    "    * Guided by information gain\n",
    "    \n",
    "#### Features\n",
    "* Simple to understand and makes some good interpretation.\n",
    "* Handles irrelevant attributes (gain=0) and missing data.\n",
    "* Faster execution time.\n",
    "\n",
    "#### Disadvantages\n",
    "* Divide data only along the axis.\n",
    "* May not find the best tree owing to greedy nature.\n",
    "* High probability of overfitting \n",
    "\n",
    "#### Major Problem\n",
    "* Overfitting: It occurs when decision trees become more specific to training data and less bias towards testing data. As a result accuracy of prediction goes down. It generally happens when it builds many branches due to outliers and irregularities in data.\n",
    "\n",
    "Two approaches to avoid overfitting:\n",
    "\n",
    "1) Pre-Pruning\n",
    "  * It stops the tree construction when the best measure of a threshold value is reached.\n",
    "  \n",
    "2) Post-Pruning (Sub Tree replacement pruning WF 6.1)\n",
    "  * Post pruning is done once the tree starts suffering from overfitting. Cross-validation check is used to measure the performance of a pruning. If there is a decrease in a accuracy during such a validation check, the node is converted into a leaf.\n",
    "        \n",
    "### Applying ID3 on given Data Set\n",
    "Estimating performace using 10-fold cross-validation technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22i7wZSeavbq"
   },
   "source": [
    "* creating instance of Decision Tree classifier algorithm\n",
    "    * Criterion: function to measure the quality of a split. (gini / entropy)\n",
    "    * max_depth (pre-prune): The max_depth parameter denotes maximum depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYUys9_RavcV"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(criterion='entropy',random_state = 100,\n",
    " max_depth=3) #creating instance of ID3 Classifier with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbceuNuMavda"
   },
   "source": [
    "* Performing 10-fold cross-validation to estimate the likely future performance of classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgRX9Dxqavdj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#applying k-fold for 10 splits, shuffle=false\n",
    "k_fold_classifn_model = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "\n",
    "id3_accuracy_score=[] # list to store accuracy for all 10 combination of data sets.\n",
    "for train_index, test_index in k_fold_classifn_model.split(X):\n",
    "    X_train=X.iloc[train_index] #using python iloc to fetch tuples for given indexes.\n",
    "    y_train=y.iloc[train_index]\n",
    "    X_test=X.iloc[test_index]\n",
    "    y_true=y.iloc[test_index]\n",
    "    \n",
    "    dtree.fit(X_train,y_train) # fit model to data set\n",
    "    y_pred = dtree.predict(X_test) # predict for outcomes\n",
    "    \n",
    "    id3_accuracy_score.append(accuracy_score(y_true, y_pred)) #computing subset accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3lzGouFaavd_"
   },
   "source": [
    "* Visualizing Decision tree (using python visualization package: pydotplus)\n",
    "\n",
    "<img src=\"data/scr3.png\" style=\"width: 600px;float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0AURMvpaveL"
   },
   "source": [
    "### 2: Naive Bayes Classification\n",
    "It is a supervised learning classification algorithm based on Bayes's Theorem. It assumes that the presence of a particular attribute in a class is independent of any other feature of the class. It makes use of prior conditional probability and Bayes's rule to predict the outcome. Naive Bayes model is very effective for the large-scale data model.\n",
    "\n",
    "**The Bayes Classifier**:\n",
    "* Conceptually, we know how frequently some particular attribute X is observed, given a known outcome. Working on the same observation we can compute the reverse, to compute the chance of that outcome happening, given X.\n",
    "* The class with the highest probability is the outcome of prediction.\n",
    "\n",
    "<img src=\"data/scr2.png\" style=\"width: 300px;float:left\"/>\n",
    "\n",
    "    P(Outcome given that we know some attributes X1,X2..) = P(X1,X2... given that we know the Outcome) times Prob(Outcome), scaled by the P(X1,X2...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**\n",
    "* It pre-computes probabilities and likelihood of training samples. Hence, classification task becomes easy and efficient.\n",
    "* It perform well in case of categorical data compared to numerical ones.\n",
    "* Popular model for real time and multi class prediction tasks.\n",
    "* Test Classification is the area where Naive Bayes is mostly used.\n",
    "        \n",
    "### Applying Naive Bayes Classifier on given Data Set\n",
    "Estimating performace using 10-fold cross-validation technique.\n",
    "\n",
    "* creating instance of Naive Bayes classifier for a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGlOuWgiaveW"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB() #creating instance of Naive Bayes Classifier with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFWOy-ExavgJ"
   },
   "source": [
    "* Performing 10-fold cross-validation to estimate the likely future performance of classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBaiBFeTavgR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#applying k-fold for 10 splits, shuffle=false\n",
    "k_fold_classifn_model = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "\n",
    "nb_accuracy_score=[] # list to store accuracy for all 10 combination of data sets.\n",
    "for train_index, test_index in k_fold_classifn_model.split(X):\n",
    "    X_train=X.iloc[train_index] #using python iloc to fetch tuples for given indexes.\n",
    "    y_train=y.iloc[train_index]\n",
    "    X_test=X.iloc[test_index]\n",
    "    y_true=y.iloc[test_index]\n",
    "    \n",
    "    nb.fit(X_train,y_train) # fit model to data set\n",
    "    y_pred = dtree.predict(X_test) # predict for outcomes\n",
    "    \n",
    "    nb_accuracy_score.append(accuracy_score(y_true, y_pred)) #computing subset accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9dTDV0pavgj"
   },
   "source": [
    "# 4) Conclusion\n",
    "\n",
    "* In the report, we used 10-fold cross-validation to split the data into train and test dataset. We estimate the likely future performance of each classification model by calculating accuracy for each fold.\n",
    "* To model decision tree classifier we used the information gain. We used a Gaussian instance for Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_VLYPuYavgv",
    "outputId": "49934783-d0b8-4466-ebdf-aeb306a364db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a ID3 model is 0.7716927453769559\n",
      "Accuracy of a Naive Bayes model is 0.8060455192034139\n"
     ]
    }
   ],
   "source": [
    "#Estimating the accuracy of machine learning model by averaging the \n",
    "#accuracies derived in all the k(=10) cases of cross validation.\n",
    "\n",
    "print (\"Accuracy of a ID3 model is \"+str(sum(id3_accuracy_score)/len(id3_accuracy_score)))\n",
    "\n",
    "print (\"Accuracy of a Naive Bayes model is \"+str(sum(nb_accuracy_score)/len(nb_accuracy_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWMBW-6savhN"
   },
   "source": [
    "<img src=\"data/scr6.png\" style=\"width: 450px;float:left\" /> <img src=\"data/scr7.png\" style=\"width: 450px;float:left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9mUbsFKavhl"
   },
   "source": [
    "**Verdict**\n",
    "\n",
    "* On comparing the accuracy of ID3 and Naive Bayes classfiers, Naive Bayes emerges as the most efficient classification model for the given data set.\n",
    "\n",
    "* There is not much difference in the accuracy between the two classifiers. Decision trees are flexible and easy to understand. They just need data in the tabular structure without any pre-processing design changes. But they tend to overfit the data and requires post-pruning. Whereas Bayes can perform quite well. It uses prior probabilities to compute class variable which makes classification task very straightforward. It doesn't overfit nearly as much so there is no need to prune or process the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itIE8QTfavhw"
   },
   "source": [
    "# 4) References\n",
    "* Tools for machine learning algorithms. Scikit-learn packages and dependencies. http://scikit-learn.org Accessed 7 Oct, 2018 \n",
    "* Pydotplus library. https://pydotplus.readthedocs.io/ Accessed 7 Oct, 2018\n",
    "* Pandas Library. http://pandas.pydata.org/pandas-docs/stable/ Accessed 7 Oct, 2018\n",
    "* Scikit-learn setup. https://github.com/scikit-learn/scikit-learn Accessed 6 Oct, 2018\n",
    "* Scikit-learn information. https://en.wikipedia.org/wiki/Scikit-learn Accessed 7 Oct, 2018\n",
    "* Decision Tree: https://en.wikipedia.org/wiki/Decision_tree Accessed 7 Oct, 2018\n",
    "* Chapter 3 and Chapter 6: Mitchell, Tom, 1997. Machine Learning. International ed. McGraw-Hill.\n",
    "* Tom M. Mitchell, Machine Learning 10-701.http://www.cs.cmu.edu/~tom/10701_sp11/slides/NBayes-1-20-2011-ann.pdf Accessed 8 Oct, 2018"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
